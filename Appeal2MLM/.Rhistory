# For feature selection
install.packages("Boruta")
# For data manipulation
install.packages("tidyverse")
#For feature selection
install.packages("caret")
#For Correlation plots
install.packages("corrplot")
library("Boruta")
library("tidyverse")
library("caret")
library("corrplot")
library(MASS)
options(
scipen = 999, digits = 3
) # Turns scientific notation off
setwd("/Users/rsing198/Documents/testcode/R_Exercise/Appeal2MLM")
appeal_df <- read.csv("dataset/Appeal_final_data.csv",header=TRUE)
summary(appeal_df)
# Caret package - A complete guide to machine learning in r
##################################
set.seed(100)
options(warn=-1)
subsets <- c(1:5, 10, 15, 18)
ctrl <- rfeControl(functions = rfFuncs,
method = "repeatedcv",
repeats = 5,
verbose = FALSE)
lmProfile <- rfe(x=appeal_scaled_df[, 2:14], y=appeal_scaled_df$success,
sizes = subsets,
rfeControl = ctrl, metric="Accuracy")
#scale the data as the values are varied
preProcess_range_model <- preProcess(appeal_df, method='range')
appeal_scaled_df <- predict(preProcess_range_model, newdata = appeal_df)
ctrl <- rfeControl(functions = rfFuncs,
method = "repeatedcv",
repeats = 5,
verbose = FALSE)
lmProfile <- rfe(x=appeal_scaled_df[, 2:14], y=appeal_scaled_df$success,
sizes = subsets,
rfeControl = ctrl, metric="Accuracy")
lmProfile
##################################
#The top 5 variables (out of 13):
#  code, payer, postca, totpay, lpadj
#####################################
print(lmProfile)
predictors(lmProfile)
ggplot(lmProfile, type=c("g","o")) +
scale_x_continuous(breaks=2:14, labels=names(appeal_scaled_df)[2:14])
# base intercept only model
model.null <- glm(success ~ 1 , data= appeal_scaled_df,
family=binomial(link="logit"))
summary(model.null)
# full model with all predictors
model.full <- glm(success ~ . , data= appeal_scaled_df,
family=binomial(link="logit"))
summary(model.full)
# Step-wise selection
model.step <- step(model.null,
scope = list(lower = model.null,
upper = model.full),
direction = "both",
test="Chisq", trace = 1, steps = 1000)
summary(model.step)
model.step
# get the shortlisted variable
shortlistedVars <- names(unlist(model.step[[1]]))
#shortlistedVars
# remove intercept
shortlistedVars <- shortlistedVars[!shortlistedVars %in% "(Intercept)"]
print(shortlistedVars)
model.selected = glm(formula=success ~ code + payer + totadj + lpadj + fpadj +
io+postca +los,
family = binomial(link = "logit"),
data = appeal_scaled_df)
summary(model.selected)
##############################################################################################################################
##################Use MASS package##########################################################################################
####http://www.sthda.com/english/articles/36-classification-methods-essentials/150-stepwise-logistic-regression-essentials-in-r/
##############################################################################################################################
model.full <- glm(success ~ . , data= appeal_scaled_df,
family=binomial)
coef(model.full)
model.full
step.model <- model.full %>% stepAIC(trace = FALSE)
coef(step.model)
step.model
model.selected.rfe = glm(formula=success ~ code + payer + postca + lpadj + fpadj +io+los + aplamt +totadj + openday ,
family = binomial(link = "logit"),
data = appeal_scaled_df)
summary(model.selected.rfe)
##https://stats.stackexchange.com/questions/199978/optimizing-probability-thresholds-in-a-glm-model-in-caret
#####################################################################
cutoffs <- seq(0.1,0.9,0.1)
accuracy <- NULL
for (i in seq(along = cutoffs)){
prediction <- ifelse(model.selected.rfe$fitted.values >= cutoffs[i], 1, 0) #Predicting for cut-off
accuracy <- c(accuracy,length(which(appeal_scaled_df$success ==prediction))/length(prediction)*100)
}
plot(cutoffs, accuracy, pch =19,type='b',col= "steelblue",
main ="Logistic Regression", xlab="Cutoff Level", ylab = "Accuracy %")
rm(
list = ls()
) # Clears global environment of all visible objects (some hidden objects
rm(
list = ls()
) # Clears global environment of all visible objects (some hidden objects
