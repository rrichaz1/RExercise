y = "True Positive Rate (Sensitivity)")
pred <- factor(ifelse(predicted > 0.5, 1, 0))
confusionMatrix(pred, as.factor(test.d$is_payment_received_after), positive="1")
# Random Forest model
m <- randomForest(Class ~ ., data=train.d)
library(caret)
# Random Forest model
m <- randomForest(Class ~ ., data=train.d)
# library for Random Forest algorithm
install.packages("randomForest")
library("randomForest")
# Random Forest model
m <- randomForest(Class ~ ., data=train.d)
m
# Random Forest model
m <- randomForest(Class ~ ., data=train.d)
# Random Forest model
m <- randomForest(is_payment_received_after ~ ip_op_num+days_appeal_opened_after_discharge, data=train.d)
m
summary(m)
predicted <- predict(m, newdata=test.d, type="prob")
predicted <- predicted[, 2]
EvaluateModel(predicted, test.d$Class, "Random Forest")
EvaluateModel(predicted, test.d$is_payment_received_after, "Random Forest")
predicted <- predict(m, newdata=test.d, type="prob")
predicted <- predicted[, 2]
EvaluateModel(predicted, test.d$is_payment_received_after, "Random Forest")
EvaluateModel(predicted, test.d$is_payment_received_after, "Random Forest")
EvaluateModel <- function(predicted, test.label, method.name) {
prediction.obj <- prediction(predicted, test.label)
perf <- performance(prediction.obj, "tpr", "fpr")
auc <- performance(prediction.obj, "auc")
# Generate ROC curve.
plot(perf)
legend("topleft", legend=paste("AUC=", round(auc@y.values[[1]], digits=4), sep=""), bty="n", cex=2)
# Generate confusion matrix and other model statistics.
pred <- factor(ifelse(predicted > 0.5, 1, 0))
confusionMatrix(pred, test.label, positive="1")
}
EvaluateModel(predicted, test.d$is_payment_received_after, "Random Forest")
summary(appeal_df)
# Logistic regression model
# Train model with training data.
m <- glm(is_payment_received_after ~ ip_op_num+days_appeal_opened_after_discharge+APPEALED_AMT, family=binomial(link="logit"), data=train.d)
# Get model details and summary.
m
summary(m)
# Evaluate model performance with test data.
predicted <- predict(m, newdata=test.d, type="response")
EvaluateModel(predicted, test.d$is_payment_received_after, "Logistic Regression")
appeal_df <- fread("/Users/singhr/Documents/testcode/R_Exercise/Appeal/Appeal_xdapost1.csv")
summary(appeal_df)
#appeal_df <- rbind(appeal_i_sample,appeal_o_sample)
#dim(appeal_df)
appeal_df$IP_OP_IND <- as.factor(appeal_df$IP_OP_IND)
appeal_df$cust_ipop <- as.factor(appeal_df$cust_ipop)
#use  variable cust_ipop to help partition data
#appeal_df = transform(appeal_df, pay_ipop = factor(paste(is_payment_received_after,IP_OP_IND)))
#summary(appeal_df)
#appeal_df$pay_ipop <- as.factor(appeal_df$pay_ipop)
#y1 <- appeal_df[, pay_ipop]
appeal_df$is_payment_received_after <- as.factor(appeal_df$is_payment_received_after)
in.train <- createDataPartition(appeal_df$is_payment_received_after, p=0.8, list=F)
#table(y1)
#table(in.train)
dim(in.train)
#dim(y1)
train.d <- appeal_df[in.train, ]
table(train.d$pay_ipop)
test.d <- appeal_df[-in.train, ]
dim(train.d)
dim(test.d)
EvaluateModel <- function(predicted, test.label, method.name) {
prediction.obj <- prediction(predicted, test.label)
perf <- performance(prediction.obj, "tpr", "fpr")
auc <- performance(prediction.obj, "auc")
# Generate ROC curve.
plot(perf)
legend("topleft", legend=paste("AUC=", round(auc@y.values[[1]], digits=4), sep=""), bty="n", cex=2)
# Generate confusion matrix and other model statistics.
pred <- factor(ifelse(predicted > 0.5, 1, 0))
confusionMatrix(pred, test.label, positive="1")
}
# Logistic regression model
# Train model with training data.
m <- glm(is_payment_received_after ~ ip_op_num+days_appeal_opened_after_discharge, family=binomial(link="logit"), data=train.d)
# Get model details and summary.
m
summary(m)
# Evaluate model performance with test data.
predicted <- predict(m, newdata=test.d, type="response")
EvaluateModel(predicted, test.d$is_payment_received_after, "Logistic Regression")
# Logistic regression model
# Train model with training data.
m <- glm(is_payment_received_after ~ ip_op_num+days_appeal_opened_after_discharge, family=binomial(link="logit"), data=train.d)
# Get model details and summary.
m
summary(m)
# Evaluate model performance with test data.
predicted <- predict(m, newdata=test.d, type="response")
EvaluateModel <- function(predicted, test.label, method.name) {
prediction.obj <- prediction(predicted, test.label)
perf <- performance(prediction.obj, "tpr", "fpr")
auc <- performance(prediction.obj, "auc")
# Generate ROC curve.
plot(perf)
legend("topleft", legend=paste("AUC=", round(auc@y.values[[1]], digits=4), sep=""), bty="n", cex=2)
# Generate confusion matrix and other model statistics.
pred <- factor(ifelse(predicted > 0.6, 1, 0))
confusionMatrix(pred, test.label, positive="1")
}
EvaluateModel(predicted, test.d$is_payment_received_after, "Random Forest")
mod_fit <- train(is_payment_received_after ~ ip_op_num+days_appeal_opened_after_discharge, data=train.d,method="glm", family="binomial")
exp(coef(mod_fit$finalModel))
appeal_df <- fread("/Users/singhr/Documents/testcode/R_Exercise/Appeal/Appeal_xdapost1.csv")
summary(appeal_df)
appeal_df %>% filter( !is.na(days_appeal_opened_after_discharge))
#appeal_df <- rbind(appeal_i_sample,appeal_o_sample)
#dim(appeal_df)
appeal_df$IP_OP_IND <- as.factor(appeal_df$IP_OP_IND)
appeal_df$cust_ipop <- as.factor(appeal_df$cust_ipop)
#use  variable cust_ipop to help partition data
#appeal_df = transform(appeal_df, pay_ipop = factor(paste(is_payment_received_after,IP_OP_IND)))
#summary(appeal_df)
#appeal_df$pay_ipop <- as.factor(appeal_df$pay_ipop)
#y1 <- appeal_df[, pay_ipop]
appeal_df$is_payment_received_after <- as.factor(appeal_df$is_payment_received_after)
in.train <- createDataPartition(appeal_df$is_payment_received_after, p=0.8, list=F)
#table(y1)
#table(in.train)
dim(in.train)
#dim(y1)
train.d <- appeal_df[in.train, ]
table(train.d$pay_ipop)
test.d <- appeal_df[-in.train, ]
dim(train.d)
dim(test.d)
EvaluateModel <- function(predicted, test.label, method.name) {
prediction.obj <- prediction(predicted, test.label)
perf <- performance(prediction.obj, "tpr", "fpr")
auc <- performance(prediction.obj, "auc")
# Generate ROC curve.
plot(perf)
legend("topleft", legend=paste("AUC=", round(auc@y.values[[1]], digits=4), sep=""), bty="n", cex=2)
# Generate confusion matrix and other model statistics.
pred <- factor(ifelse(predicted > 0.6, 1, 0))
confusionMatrix(pred, test.label, positive="1")
}
# Logistic regression model
# Train model with training data.
m <- glm(is_payment_received_after ~ ip_op_num+days_appeal_opened_after_discharge, family=binomial(link="logit"), data=train.d)
# Get model details and summary.
m
summary(m)
mod_fit <- train(is_payment_received_after ~ ip_op_num+days_appeal_opened_after_discharge, data=train.d,method="glm", family="binomial")
is.na(appeal_df$is_payment_received_after)
appeal_df %>% filter( !is.na(days_appeal_opened_after_discharge & !is.na(appeal_df$is_payment_received_after)))
appeal_df %>% filter( !is.na(days_appeal_opened_after_discharge & !is.na(appeal_df$is_payment_received_after)))
is.na(appeal_df$is_payment_received_after)
summary(appeal_df)
appeal_df <- fread("/Users/singhr/Documents/testcode/R_Exercise/Appeal/Appeal_xdapost1.csv")
summary(appeal_df)
appeal_df %>% filter( !is.na(days_appeal_opened_after_discharge))
summary(appeal_df)
appeal_df %>% filter( !is.na(days_appeal_opened_after_discharge))
is.na(appeal_df$is_payment_received_after)
#use  variable cust_ipop to help partition data
#appeal_df = transform(appeal_df, pay_ipop = factor(paste(is_payment_received_after,IP_OP_IND)))
#summary(appeal_df)
#appeal_df$pay_ipop <- as.factor(appeal_df$pay_ipop)
#y1 <- appeal_df[, pay_ipop]
appeal_df$is_payment_received_after <- as.factor(appeal_df$is_payment_received_after)
in.train <- createDataPartition(appeal_df$is_payment_received_after, p=0.8, list=F)
#table(y1)
#table(in.train)
dim(in.train)
#dim(y1)
train.d <- appeal_df[in.train, ]
table(train.d$pay_ipop)
test.d <- appeal_df[-in.train, ]
dim(train.d)
dim(test.d)
EvaluateModel <- function(predicted, test.label, method.name) {
prediction.obj <- prediction(predicted, test.label)
perf <- performance(prediction.obj, "tpr", "fpr")
auc <- performance(prediction.obj, "auc")
# Generate ROC curve.
plot(perf)
legend("topleft", legend=paste("AUC=", round(auc@y.values[[1]], digits=4), sep=""), bty="n", cex=2)
# Generate confusion matrix and other model statistics.
pred <- factor(ifelse(predicted > 0.6, 1, 0))
confusionMatrix(pred, test.label, positive="1")
}
# Logistic regression model
# Train model with training data.
m <- glm(is_payment_received_after ~ ip_op_num+days_appeal_opened_after_discharge, family=binomial(link="logit"), data=train.d)
# Get model details and summary.
m
summary(m)
mod_fit <- train(is_payment_received_after ~ ip_op_num+days_appeal_opened_after_discharge, data=train.d,method="glm", family="binomial")
missing <- is.na(appeal_df)
which(rowSums(missing) == 1)
appeal_df <- appeal_df[rowSums(is.na(appeal_df))==0, ]
summary(appeal_df)
appeal_df <- select (appeal_df,
days_appeal_opened_after_discharge,IP_OP_IND,
is_payment_received_after,
ip_op_num)
summary(appeal_df)
appeal_df <- fread("/Users/singhr/Documents/testcode/R_Exercise/Appeal/Appeal_xdapost1.csv")
appeal_df <- select (appeal_df,
days_appeal_opened_after_discharge,IP_OP_IND,
is_payment_received_after,
ip_op_num)
summary(appeal_df)
missing <- is.na(appeal_df)
which(rowSums(missing) == 1)
appeal_df <- appeal_df[rowSums(is.na(appeal_df))==0, ]
summary(appeal_df)
appeal_df %>% filter( !is.na(days_appeal_opened_after_discharge))
is.na(appeal_df$is_payment_received_after)
#appeal_df <- rbind(appeal_i_sample,appeal_o_sample)
#dim(appeal_df)
appeal_df$IP_OP_IND <- as.factor(appeal_df$IP_OP_IND)
#use  variable cust_ipop to help partition data
#appeal_df = transform(appeal_df, pay_ipop = factor(paste(is_payment_received_after,IP_OP_IND)))
#summary(appeal_df)
#appeal_df$pay_ipop <- as.factor(appeal_df$pay_ipop)
#y1 <- appeal_df[, pay_ipop]
appeal_df$is_payment_received_after <- as.factor(appeal_df$is_payment_received_after)
in.train <- createDataPartition(appeal_df$is_payment_received_after, p=0.8, list=F)
#table(y1)
#table(in.train)
dim(in.train)
#dim(y1)
train.d <- appeal_df[in.train, ]
table(train.d$pay_ipop)
test.d <- appeal_df[-in.train, ]
dim(train.d)
dim(test.d)
EvaluateModel <- function(predicted, test.label, method.name) {
prediction.obj <- prediction(predicted, test.label)
perf <- performance(prediction.obj, "tpr", "fpr")
auc <- performance(prediction.obj, "auc")
# Generate ROC curve.
plot(perf)
legend("topleft", legend=paste("AUC=", round(auc@y.values[[1]], digits=4), sep=""), bty="n", cex=2)
# Generate confusion matrix and other model statistics.
pred <- factor(ifelse(predicted > 0.6, 1, 0))
confusionMatrix(pred, test.label, positive="1")
}
# Logistic regression model
# Train model with training data.
m <- glm(is_payment_received_after ~ ip_op_num+days_appeal_opened_after_discharge, family=binomial(link="logit"), data=train.d)
# Get model details and summary.
m
summary(m)
mod_fit <- train(is_payment_received_after ~ ip_op_num+days_appeal_opened_after_discharge, data=train.d,method="glm", family="binomial")
exp(coef(mod_fit$finalModel))
predict(mod_fit, newdata=test.d, type="prob")
m2 <- glm(is_payment_received_after ~ days_appeal_opened_after_discharge, family=binomial(link="logit"), data=train.d)
m2
# Get model details and summary.
m
#Compare m and m2 to see which is better fit
anova(m, m2, test ="Chisq")
lrtest(m, m2)
ibrary(lmtest)
lrtest(m, m2)
library(lmtest)
lrtest(m, m2)
library(lrtest)
install.packages(lrtest)
install.packages(lmtest)
install.packages("lmtest")
install.packages("lmtest")
varImp(m)
#K-Fold Cross Validation
ctrl <- trainControl(method = "repeatedcv", number = 10, savePredictions = TRUE)
mod_fit <- train(is_payment_received_after ~ ip_op_num+days_appeal_opened_after_discharge, data=train.d,method="glm",
family="binomial",
trControl = ctrl, tuneLength = 5)
mod_fit <- train(is_payment_received_after ~ ip_op_num+days_appeal_opened_after_discharge, data=train.d,method="glm",
family="binomial",
trControl = ctrl, tuneLength = 5)
pred = predict(mod_fit, newdata=newdata=test.d)
confusionMatrix(data=pred, newdata=test.d$is_payment_received_after)
pred <- predict(mod_fit, newdata=newdata=test.d)
pred =predict(mod_fit, newdata=test.d)
confusionMatrix(data=pred, newdata=test.d$is_payment_received_after)
confusionMatrix(data=pred, newdata=test.d$is_payment_received_after,positive="1")
pred
?confusionMatrix
?confusionMatrix
test.d <- appeal_df[-in.train, ]
dim(train.d)
dim(test.d)
confusionMatrix(data=pred, newdata=test.d$is_payment_received_after,positive="1")
#pred <- factor(ifelse(predicted > 0.6, 1, 0))
confusionMatrix(data=factor(pred), newdata=test.d$is_payment_received_after,positive="1")
# Evaluate model performance with test data.
predicted <- predict(m, newdata=test.d, type="response")
pred <- factor(ifelse(predicted > 0.6, 1, 0))
confusionMatrix(pred, test.label, positive="1")
?confusionMatrix
install.packages("tidyverse")  #tidyerse has ggplot2 and dplyr
install.packages("readr")
install.packages("ROCR")
devtools::install_github("sachsmc/plotROC")
library(plotROC)
install.packages("tidyverse")
# library for Random Forest algorithm
install.packages("randomForest")
library("randomForest")
library("data.table")  # has fread
library("ggplot2")
library(caret)
require("tidyverse")
library("ROCR")
install.packages("ROCR")
install.packages("ROCR")
install.packages("ROCR")
install.packages("ROCR")
install.packages("ROCR")
install.packages("ROCR")
install.packages("ROCR")
install.packages("ROCR")
install.packages("tidyverse")  #tidyerse has ggplot2 and dplyr
install.packages("readr")
install.packages("ROCR")
install.packages("ROCR")
install.packages("ROCR")
install.packages("ROCR")
# library for Random Forest algorithm
install.packages("randomForest")
library("randomForest")
library("data.table")  # has fread
library("ggplot2")
require("tidyverse")
library(caret)
library("ROCR")
appeal_df <- fread("/Users/singhr/Documents/testcode/R_Exercise/Appeal/Appeal_xdapost1.csv")
appeal_df <- select (appeal_df,
days_appeal_opened_after_discharge,IP_OP_IND,
is_payment_received_after,
ip_op_num)
summary(appeal_df)
missing <- is.na(appeal_df)
which(rowSums(missing) == 1)
appeal_df <- appeal_df[rowSums(is.na(appeal_df))==0, ]
appeal_df %>% filter( !is.na(days_appeal_opened_after_discharge))
#use  variable cust_ipop to help partition data
#appeal_df = transform(appeal_df, pay_ipop = factor(paste(is_payment_received_after,IP_OP_IND)))
#summary(appeal_df)
#appeal_df$pay_ipop <- as.factor(appeal_df$pay_ipop)
#y1 <- appeal_df[, pay_ipop]
appeal_df$is_payment_received_after <- as.factor(appeal_df$is_payment_received_after)
in.train <- createDataPartition(appeal_df$is_payment_received_after, p=0.8, list=F)
#table(y1)
#table(in.train)
dim(in.train)
#dim(y1)
train.d <- appeal_df[in.train, ]
table(train.d$pay_ipop)
test.d <- appeal_df[-in.train, ]
dim(train.d)
dim(test.d)
EvaluateModel <- function(predicted, test.label, method.name) {
prediction.obj <- prediction(predicted, test.label)
perf <- performance(prediction.obj, "tpr", "fpr")
auc <- performance(prediction.obj, "auc")
# Generate ROC curve.
plot(perf)
legend("topleft", legend=paste("AUC=", round(auc@y.values[[1]], digits=4), sep=""), bty="n", cex=2)
# Generate confusion matrix and other model statistics.
pred <- factor(ifelse(predicted > 0.6, 1, 0))
confusionMatrix(pred, test.label, positive="1")
}
# Logistic regression model
# Train model with training data.
m <- glm(is_payment_received_after ~ ip_op_num+days_appeal_opened_after_discharge, family=binomial(link="logit"), data=train.d)
m2 <- glm(is_payment_received_after ~ days_appeal_opened_after_discharge, family=binomial(link="logit"), data=train.d)
# Get model details and summary.
m
m2
summary(m)
#Compare m and m2 to see which is better fit
anova(m, m2, test ="Chisq")
varImp(m)
#See coefficients as exponents
mod_fit <- train(is_payment_received_after ~ ip_op_num+days_appeal_opened_after_discharge, data=train.d,method="glm", family="binomial")
exp(coef(mod_fit$finalModel))
predict(mod_fit, newdata=test.d, type="prob")
# Evaluate model performance with test data.
predicted <- predict(m, newdata=test.d, type="response")
pred <- factor(ifelse(predicted > 0.6, 1, 0))
EvaluateModel(predicted, test.d$is_payment_received_after, "Logistic Regression")
#K-Fold Cross Validation
ctrl <- trainControl(method = "repeatedcv", number = 10, savePredictions = TRUE)
mod_fit <- train(is_payment_received_after ~ ip_op_num+days_appeal_opened_after_discharge, data=train.d,method="glm",
family="binomial",
trControl = ctrl, tuneLength = 5)
pred =predict(mod_fit, newdata=test.d)
#pred <- factor(ifelse(predicted > 0.6, 1, 0))
confusionMatrix(data=pred, newdata=test.d$is_payment_received_after,positive="1")
pred =predict(mod_fit, newdata=test.d, type="prob")
EvaluateModel(pred, test.d$is_payment_received_after, "Logistic K Fold Regression")
dim(pred)
dim(test.d)
#K-Fold Cross Validation
ctrl <- trainControl(method = "repeatedcv", number = 10, savePredictions = TRUE)
mod_fit <- train(is_payment_received_after ~ ip_op_num+days_appeal_opened_after_discharge, data=train.d,method="glm",
family="binomial",
trControl = ctrl, tuneLength = 5)
pred =predict(mod_fit, newdata=test.d, type="prob")
dim(pred)
pred
EvaluateModel <- function(predicted, test.label, method.name) {
prediction.obj <- prediction(predicted, test.label)
perf <- performance(prediction.obj, "tpr", "fpr")
auc <- performance(prediction.obj, "auc")
# Generate ROC curve.
plot(perf)
legend("topleft", legend=paste("AUC=", round(auc@y.values[[1]], digits=4), sep=""), bty="n", cex=2)
# Generate confusion matrix and other model statistics.
pred <- factor(ifelse(predicted > 0.5, 1, 0))
confusionMatrix(pred, test.label, positive="1")
}
EvaluateModel(pred, test.d$is_payment_received_after, "Logistic K Fold Regression")
predicted1 =predict(mod_fit, newdata=test.d, type="prob")
pred <- factor(ifelse(predicted > 0.6, 1, 0))
pred <- factor(ifelse(predicted1 > 0.6, 1, 0))
confusionMatrix(data=pred, newdata=test.d$is_payment_received_after, positive="1")
?confusionMatrix
install.packages("tidyverse")  #tidyerse has ggplot2 and dplyr
install.packages("readr")
library("data.table")  # has fread
library("ggplot2")
require("tidyverse")
library(caret)
appeal_df <- fread("/Users/singhr/Documents/testcode/R_Exercise/Appeal/Appeal_xdapost1.csv")
appeal_df <- select (appeal_df,
days_appeal_opened_after_discharge,IP_OP_IND,
is_payment_received_after,
ip_op_num)
?confusionMatrix
summary(appeal_df)
missing <- is.na(appeal_df)
which(rowSums(missing) == 1)
appeal_df <- appeal_df[rowSums(is.na(appeal_df))==0, ]
appeal_df %>% filter( !is.na(days_appeal_opened_after_discharge))
#use  variable cust_ipop to help partition data
#appeal_df = transform(appeal_df, pay_ipop = factor(paste(is_payment_received_after,IP_OP_IND)))
#summary(appeal_df)
#appeal_df$pay_ipop <- as.factor(appeal_df$pay_ipop)
#y1 <- appeal_df[, pay_ipop]
appeal_df$is_payment_received_after <- as.factor(appeal_df$is_payment_received_after)
in.train <- createDataPartition(appeal_df$is_payment_received_after, p=0.8, list=F)
#table(y1)
#table(in.train)
dim(in.train)
#dim(y1)
train.d <- appeal_df[in.train, ]
table(train.d$pay_ipop)
test.d <- appeal_df[-in.train, ]
dim(train.d)
dim(test.d)
EvaluateModel <- function(predicted, test.label, method.name) {
prediction.obj <- prediction(predicted, test.label)
perf <- performance(prediction.obj, "tpr", "fpr")
auc <- performance(prediction.obj, "auc")
# Generate ROC curve.
plot(perf)
legend("topleft", legend=paste("AUC=", round(auc@y.values[[1]], digits=4), sep=""), bty="n", cex=2)
# Generate confusion matrix and other model statistics.
pred <- factor(ifelse(predicted > 0.5, 1, 0))
confusionMatrix(pred, test.label, positive="1")
}
#K-Fold Cross Validation
ctrl <- trainControl(method = "repeatedcv", number = 10, savePredictions = TRUE)
mod_fit <- train(is_payment_received_after ~ ip_op_num+days_appeal_opened_after_discharge, data=train.d,method="glm",
family="binomial",
trControl = ctrl, tuneLength = 5)
predicted1 =predict(mod_fit, newdata=test.d, type="prob")
pred <- factor(ifelse(predicted1 > 0.5, 1, 0))
confusionMatrix(data=pred, test.d$is_payment_received_after, positive="1")
confusionMatrix(pred, test.d$is_payment_received_after, positive="1")
pred
dim(pred)
summary(pred)
summary(predicted1)
predicted1 =predict(mod_fit, newdata=test.d)
summary(predicted1)
#pred <- factor(ifelse(predicted1 > 0.5, 1, 0))
confusionMatrix(pred, test.d$is_payment_received_after, positive="1")
#pred <- factor(ifelse(predicted1 > 0.5, 1, 0))
confusionMatrix(predicted1, test.d$is_payment_received_after, positive="1")
mod_fit <- train(is_payment_received_after ~ ip_op_num+days_appeal_opened_after_discharge, data=train.d,method="glm",
family="binomial",
trControl = ctrl, tuneLength = 5)
predicted1 =predict(mod_fit, newdata=test.d)
#pred <- factor(ifelse(predicted1 > 0.5, 1, 0))
confusionMatrix(predicted1, test.d$is_payment_received_after, positive="1")
Auto=read.table("Auto.data")
library(MASS)
library(ISLR)
Auto=read.table("Auto.data")
setwd("~/Documents/testcode/R_Exercise/ML/StatLearning")
Auto=read.table("Auto.data")
Auto=read.table("Auto.data")
fix(Auto)
Auto
