library("devtools")
install.packages("car")
install.packages("ROCR")#For calculating predicted values
install.packages("e1071")
install.packages("MLmetrics")
install.packages("caret")
install.packages("tidyverse")
devtools::install_github("ujjwalkarn/xda")
library("tidyverse")
library("ggplot2")
library("xda")
library("ROCR")
library("MLmetrics")
library("caret")
setwd("/Users/rsing198/Documents/testcode/R_Exercise/Surge/Pre")
trainDS <- read.csv("trainingData.csv",header=TRUE)
str(trainDS)
summary(trainDS)
trainDS <- trainDS %>% select(-c("claims_daysaway", "testindex"))
trainDSNoFactor <-trainDS
trainData <- trainDSNoFactor %>% drop_na %>% group_by(outcome) %>% sample_n(6800)
summary(trainData)
set.seed(100)
# Step 1: Get row numbers for the training data
trainRowNumbers <- createDataPartition(trainData$outcome, p=0.7, list=FALSE)
# Step 2: Create the training dataset
trainData <- trainData[trainRowNumbers,]
# Step 3: Create the test dataset
testData <- trainData[-trainRowNumbers,]
#Randomize
train.data.random <- sample_frac(trainData, 1L)
test.data.random <- sample_frac(testData, 1L)
str(train.data.random)
View(train.data.random)
#Randomize
train.data.random <- trainData[sample(nrow(trainData)),]
test.data.random <- testData[sample(nrow(testData)),]
View(train.data.random)
str(train.data.random)
str(test.data.random)
EvaluateModel <- function(predicted, test.label, method.name, threshold) {
if (is.factor(predicted))
{
predicted <- as.numeric(predicted) -1
}
prediction.obj <- prediction(predicted, test.label)
perf <- performance(prediction.obj, "tpr", "fpr")
perf
auc <- performance(prediction.obj, "auc")
# Generate ROC curve.
plot(perf, colorize=F, col="blue", lwd=3, cex.main=1.5, main=method.name)
lines(c(0,1), c(0,1), col="gray", lwd=2)
legend("topleft", legend=paste("AUC=", round(auc@y.values[[1]], digits=4), sep=""), bty="n", cex=2)
# Generate confusion matrix and other model statistics.
pred <- factor(ifelse(predicted >= threshold, 1, 0))
confusionMatrix(pred, test.label, positive="1")
}
Evaluate_F1_Score<- function(predicted, test.label, method.name, threshold) {
if (is.factor(predicted))
{
predicted <- as.numeric(predicted) -1
}
pred <- factor(ifelse(predicted >= threshold, 1, 0))
F1_Score(y_pred = pred, test.label, positive = "1")
}
############
#Logistic Regression
###############
train.data <- train.data.random
model.logit <- glm(outcome ~ . , data= train.data,
family=binomial)
model.logit
#Predict on training data
fitted <- predict(model.logit,train.data)
Evaluate_F1_Score(fitted, train.data$outcome, "Logistic Regression Train", 0.5)
EvaluateModel(fitted, train.data$outcome, "Logistic Regression Train", 0.5)
set.seed(100)
#Predict on test data
fitted <- predict(model.logit,test.data)
#Predict on test data
test.data <- test.data.random
fitted <- predict(model.logit,test.data)
Evaluate_F1_Score(fitted, as.factor(test.data$outcome), "Logistic Regression Train", 0.5)
EvaluateModel(fitted,  as.factor(test.data$outcome), "Logistic Regression Train", 0.5)
set.seed(100)
fitControl <- trainControl(## 10-fold CV
method = "cv",
number = 10,
savePredictions = TRUE,
classProbs = TRUE
)
# Train the model using random forest
model.rf = train(outcome ~ ., data=train.data, method='rf', tuneLength=5, trControl = fitControl,importance=T)
model.rf
#train.data$success = as.factor(train.data$success)
#test.data$success = factor(test.data$success)
varimp_rf<- varImp(model.rf)
plot(varimp_rf, main="Variable Importance with Random Forest")
#Predict on training data
fitted <- predict(model.rf,train.data)
Evaluate_F1_Score(fitted, as.factor(train.data$outcome), "Logistic Regression Train", 0.5)
EvaluateModel(fitted,  as.factor(train.data$outcome), "Logistic Regression Train", 0.5)
#Predict on test data
test.data <- test.data.random
#Predict on test data
test.data <- test.data.random
fitted <- predict(model.logit,test.data)
Evaluate_F1_Score(fitted, as.factor(test.data$outcome), "RFE Regression Test", 0.5)
EvaluateModel(fitted,  as.factor(test.data$outcome), "RFE Regression Test", 0.5)
# Train the model using random forest
model.rf = train(outcome ~ ., data=train.data, method='rf', tuneLength=5, trControl = fitControl,importance=T,preProcess = c("center", "scale"))
model.rf
#train.data$success = as.factor(train.data$success)
#test.data$success = factor(test.data$success)
varimp_rf<- varImp(model.rf)
plot(varimp_rf, main="Variable Importance with Random Forest")
#Predict on training data
fitted <- predict(model.rf,train.data)
Evaluate_F1_Score(fitted, as.factor(train.data$outcome), "RFE Regression Train", 0.5)
EvaluateModel(fitted,  as.factor(train.data$outcome), "RFE Regression Train", 0.5)
#Predict on test data
test.data <- test.data.random
fitted <- predict(model.logit,test.data)
Evaluate_F1_Score(fitted, as.factor(test.data$outcome), "RFE Regression Test", 0.5)
fitted <- predict(model.rf,test.data)
Evaluate_F1_Score(fitted, as.factor(test.data$outcome), "RFE Regression Test", 0.5)
EvaluateModel(fitted,  as.factor(test.data$outcome), "RFE Regression Test", 0.5)
set.seed(100)
# Step 1: Get row numbers for the training data
trainRowNumbers <- createDataPartition(trainDSNoFactor$outcome, p=0.7, list=FALSE)
# Step 2: Create the training dataset
trainData <- appeal_subset_df[trainRowNumbers,]
# Step 3: Create the test dataset
testDataOrig <- trainDSNoFactor[-trainRowNumbers,]
test.dataOrig.random <- testData[sample(nrow(testDataOrig)),]
str(test.dataOrig.random)
test.data <-test.dataOrig.random
fitted <- predict(model.rf,test.data)
Evaluate_F1_Score(fitted, as.factor(test.data$outcome), "RFE Regression Test", 0.5)
trainDSNoFactorNoNA <- trainDSNoFactor %>% drop_na
# Step 1: Get row numbers for the training data
trainRowNumbers <- createDataPartition(trainDSNoFactorNoNA$outcome, p=0.7, list=FALSE)
# Step 2: Create the training dataset
#trainData <- appeal_subset_df[trainRowNumbers,]
# Step 3: Create the test dataset
testDataOrig <- trainDSNoFactorNoNA[-trainRowNumbers,]
test.dataOrig.random <- testData[sample(nrow(testDataOrig)),]
str(test.data.random)
str(test.dataOrig.random)
View(test.dataOrig.random)
trainDSNoFactorNoNA <- trainDSNoFactor %>% drop_na
View(trainDSNoFactorNoNA)
# Step 1: Get row numbers for the training data
trainRowNumbers <- createDataPartition(trainDSNoFactorNoNA$outcome, p=0.7, list=FALSE)
# Step 2: Create the training dataset
#trainData <- appeal_subset_df[trainRowNumbers,]
# Step 3: Create the test dataset
testDataOrig <- trainDSNoFactorNoNA[-trainRowNumbers,]
View(testDataOrig)
test.dataOrig.random <- testData[sample(nrow(testDataOrig)),]
test.dataOrig.random <- trainDSNoFactorNoNA[sample(nrow(testDataOrig)),]
View(test.dataOrig.random)
test.data <-test.dataOrig.random
fitted <- predict(model.rf,test.data)
Evaluate_F1_Score(fitted, as.factor(test.data$outcome), "RFE Regression Test", 0.5)
EvaluateModel(fitted,  as.factor(test.data$outcome), "RFE Regression Test", 0.5)
install.packages("car")
install.packages("ROCR")#For calculating predicted values
install.packages("e1071")
install.packages("MLmetrics")
install.packages("caret")
install.packages("tidyverse")
devtools::install_github("ujjwalkarn/xda")
library("tidyverse")
library("ggplot2")
library("xda")
library("ROCR")
library("MLmetrics")
library("caret")
setwd("/Users/rsing198/Documents/testcode/R_Exercise/Surge/Pre")
install.packages("ROCR")
install.packages("tidyverse")
devtools::install_github("ujjwalkarn/xda")
library("tidyverse")
library("ggplot2")
library("xda")
library("ROCR")
library("MLmetrics")
library("caret")
setwd("/Users/rsing198/Documents/testcode/R_Exercise/Surge/Pre")
trainDS <- read.csv("trainingData.csv",header=TRUE)
trainDS <- trainDS %>% select(-c("claims_daysaway", "testindex"))
trainDSNoFactor <-trainDS
summary(trainDS)
set.seed(100)
trainDSNoFactorNoNA <- trainDSNoFactor %>% drop_na
set.seed(100)
trainDSNoFactorNoNA <- trainDSNoFactor %>% drop_na
# Step 1: Get row numbers for the training data
trainRowNumbers <- createDataPartition(trainDSNoFactorNoNA$outcome, p=0.7, list=FALSE)
# Step 2: Create the training dataset
#trainData <- appeal_subset_df[trainRowNumbers,]
# Step 3: Create the test dataset
testDataOrig <- trainDSNoFactorNoNA[-trainRowNumbers,]
test.dataOrig.random <- trainDSNoFactorNoNA[sample(nrow(testDataOrig)),]
trainData <- trainDSNoFactor %>% drop_na %>% group_by(outcome) %>% sample_n(6800)
summary(trainData)
###########################################################################################
##OVERSAMPLING
trainData <- trainDSNoFactor %>% drop_na %>% group_by(outcome) %>% sample_n(6800)
summary(trainData)
tab_sameadd <- table(trainData$outcome, trainData$same_address)
tab_sameadd
prop.table(tab_sameadd)
summary(prop.table(tab_sameadd))
chisq.test(tab_sameadd,p=0.05)
cramersV(tab_sameadd)
##############################################################
# Create the training and test datasets
##############################################################
set.seed(100)
# Step 1: Get row numbers for the training data
trainRowNumbers <- createDataPartition(trainData$outcome, p=0.7, list=FALSE)
# Step 2: Create the training dataset
trainData <- trainData[trainRowNumbers,]
# Step 3: Create the test dataset
testData <- trainData[-trainRowNumbers,]
#Randomize
train.data.random <- trainData[sample(nrow(trainData)),]
test.data.random <- testData[sample(nrow(testData)),]
set.seed(100)
trainDSNoFactorNoNA <- trainDSNoFactor %>% drop_na
# Step 1: Get row numbers for the training data
trainRowNumbers <- createDataPartition(trainDSNoFactorNoNA$outcome, p=0.7, list=FALSE)
# Step 2: Create the training dataset
#trainData <- appeal_subset_df[trainRowNumbers,]
# Step 3: Create the test dataset
testDataOrig <- trainDSNoFactorNoNA[-trainRowNumbers,]
test.dataOrig.random <- trainDSNoFactorNoNA[sample(nrow(testDataOrig)),]
str(train.data.random)
str(test.data.random)
str(test.dataOrig.random)
EvaluateModel <- function(predicted, test.label, method.name, threshold) {
if (is.factor(predicted))
{
predicted <- as.numeric(predicted) -1
}
prediction.obj <- prediction(predicted, test.label)
perf <- performance(prediction.obj, "tpr", "fpr")
perf
auc <- performance(prediction.obj, "auc")
# Generate ROC curve.
plot(perf, colorize=F, col="blue", lwd=3, cex.main=1.5, main=method.name)
lines(c(0,1), c(0,1), col="gray", lwd=2)
legend("topleft", legend=paste("AUC=", round(auc@y.values[[1]], digits=4), sep=""), bty="n", cex=2)
# Generate confusion matrix and other model statistics.
pred <- factor(ifelse(predicted >= threshold, 1, 0))
confusionMatrix(pred, test.label, positive="1")
}
Evaluate_F1_Score<- function(predicted, test.label, method.name, threshold) {
if (is.factor(predicted))
{
predicted <- as.numeric(predicted) -1
}
pred <- factor(ifelse(predicted >= threshold, 1, 0))
F1_Score(y_pred = pred, test.label, positive = "1")
}
############
#Logistic Regression
###############
train.data <- train.data.random
model.logit <- glm(outcome ~ . , data= train.data,
family=binomial)
model.logit
#Predict on training data
fitted <- predict(model.logit,train.data)
Evaluate_F1_Score(fitted, as.factor(train.data$outcome), "Logistic Regression Train", 0.5)
EvaluateModel(fitted,  as.factor(train.data$outcome), "Logistic Regression Train", 0.5)
#Predict on test data
test.data <- test.data.random
fitted <- predict(model.logit,test.data)
Evaluate_F1_Score(fitted, as.factor(test.data$outcome), "Logistic Regression Test", 0.5)
EvaluateModel(fitted,  as.factor(test.data$outcome), "Logistic Regression Test", 0.5)
set.seed(100)
fitControl <- trainControl(## 10-fold CV
method = "cv",
number = 10,
savePredictions = TRUE
#  classProbs = TRUE
)
# Train the model using random forest
model.rf = train(outcome ~ ., data=train.data, method='rf', tuneLength=5, trControl = fitControl,importance=T,preProcess = c("center", "scale"))
columns <- c('outcome', 'fqhc','tier','pcp_lookback','family_assignment','kid',
'is_ped','same_gender','same_language','same_address','distance','visit_count')
model.rf
#train.data$success = as.factor(train.data$success)
#test.data$success = factor(test.data$success)
varimp_rf<- varImp(model.rf)
plot(varimp_rf, main="Variable Importance with Random Forest")
#Predict on training data
fitted <- predict(model.rf,train.data)
Evaluate_F1_Score(fitted, as.factor(train.data$outcome), "RFE Regression Train", 0.5)
EvaluateModel(fitted,  as.factor(train.data$outcome), "RFE Regression Train", 0.5)
#Predict on test data
test.data <- test.data.random
fitted <- predict(model.rf,test.data)
Evaluate_F1_Score(fitted, as.factor(test.data$outcome), "RFE Regression Test", 0.5)
EvaluateModel(fitted,  as.factor(test.data$outcome), "RFE Regression Test", 0.5)
test.data <-test.dataOrig.random
fitted <- predict(model.rf,test.data)
Evaluate_F1_Score(fitted, as.factor(test.data$outcome), "RFE Regression Test", 0.5)
EvaluateModel(fitted,  as.factor(test.data$outcome), "RFE Regression Test", 0.5)
str(test.data)
str(fitted)
str(fitted.values)
summary(fitted)
str(test.data)
test.data <-test.dataOrig.random
fitted <- predict(model.rf,test.data)
table(fitted)
pred <- factor(ifelse(fitted >= 0.5, 1, 0))
pred
table(pred)
scoreDS <- read.csv("scoringData.csv",header=TRUE)
str(scoreDS)
summary(scoreDS)
scoreDSNoNA <- scoreDS %>% drop_na
scoreDSNoIndex <- scoreDSNoNA %>% select(columns)
columns <- c( 'fqhc','tier','pcp_lookback','family_assignment','kid',
'is_ped','same_gender','same_language','same_address','distance','visit_count')
scoreDSNoIndex <- scoreDSNoNA %>% select(columns)
str(scoreDSNoIndex)
str(test.data)
(
str(scoreDS)
str(scoreDS)
str(scoreDS)
scoreDSNoNA <- scoreDS %>% select(-c("claims_daysaway")) %>% drop_na
str(scoreDSNoNA)
str(trainDS)
missing <- is.na(trainDS)
which(rowSums(missing) == 1)
numSummary(trainDS)
trainDS[is_na(trainDS)] <- 1
trainDS[is.na(trainDS)] <- 1
numSummary(trainDS)
trainDSNoFactor <-trainDS
###########################################################################################
##OVERSAMPLING
trainData <- trainDSNoFactor %>% drop_na %>% group_by(outcome) %>% sample_n(6800)
summary(trainData)
str(trainData)
str(trainDSNoFactor)
##############################################################
# Create the training and test datasets
##############################################################
set.seed(100)
# Step 1: Get row numbers for the training data
trainRowNumbers <- createDataPartition(trainData$outcome, p=0.7, list=FALSE)
# Step 2: Create the training dataset
trainData <- trainData[trainRowNumbers,]
# Step 3: Create the test dataset
testData <- trainData[-trainRowNumbers,]
#Randomize
train.data.random <- trainData[sample(nrow(trainData)),]
test.data.random <- testData[sample(nrow(testData)),]
set.seed(100)
trainDSNoFactorNoNA <- trainDSNoFactor %>% drop_na
# Step 1: Get row numbers for the training data
trainRowNumbers <- createDataPartition(trainDSNoFactorNoNA$outcome, p=0.7, list=FALSE)
str(trainDSNoFactorNoNA)
# Step 1: Get row numbers for the training data
trainRowNumbers <- createDataPartition(trainDSNoFactorNoNA$outcome, p=0.7, list=FALSE)
# Step 2: Create the training dataset
#trainData <- appeal_subset_df[trainRowNumbers,]
# Step 3: Create the test dataset
testDataOrig <- trainDSNoFactorNoNA[-trainRowNumbers,]
test.dataOrig.random <- trainDSNoFactorNoNA[sample(nrow(testDataOrig)),]
str(train.data.random)
str(test.data.random)
str(test.dataOrig.random)
EvaluateModel <- function(predicted, test.label, method.name, threshold) {
if (is.factor(predicted))
{
predicted <- as.numeric(predicted) -1
}
prediction.obj <- prediction(predicted, test.label)
perf <- performance(prediction.obj, "tpr", "fpr")
perf
auc <- performance(prediction.obj, "auc")
# Generate ROC curve.
plot(perf, colorize=F, col="blue", lwd=3, cex.main=1.5, main=method.name)
lines(c(0,1), c(0,1), col="gray", lwd=2)
legend("topleft", legend=paste("AUC=", round(auc@y.values[[1]], digits=4), sep=""), bty="n", cex=2)
# Generate confusion matrix and other model statistics.
pred <- factor(ifelse(predicted >= threshold, 1, 0))
confusionMatrix(pred, test.label, positive="1")
}
Evaluate_F1_Score<- function(predicted, test.label, method.name, threshold) {
if (is.factor(predicted))
{
predicted <- as.numeric(predicted) -1
}
pred <- factor(ifelse(predicted >= threshold, 1, 0))
F1_Score(y_pred = pred, test.label, positive = "1")
}
set.seed(100)
fitControl <- trainControl(## 10-fold CV
method = "cv",
number = 10,
savePredictions = TRUE
#  classProbs = TRUE
)
# Train the model using random forest
model.rf = train(outcome ~ ., data=train.data, method='rf', tuneLength=5, trControl = fitControl,importance=T,preProcess = c("center", "scale"))
model.rf
#train.data$success = as.factor(train.data$success)
#test.data$success = factor(test.data$success)
varimp_rf<- varImp(model.rf)
plot(varimp_rf, main="Variable Importance with Random Forest")
#Predict on training data
fitted <- predict(model.rf,train.data)
Evaluate_F1_Score(fitted, as.factor(train.data$outcome), "RFE Regression Train", 0.5)
EvaluateModel(fitted,  as.factor(train.data$outcome), "RFE Regression Train", 0.5)
fitted
Evaluate_F1_Score(fitted, as.factor(train.data$outcome), "RFE Regression Train", 0.5)
EvaluateModel(fitted,  as.factor(train.data$outcome), "RFE Regression Train", 0.5)
#Predict on test data
test.data <- test.data.random
fitted <- predict(model.rf,test.data)
Evaluate_F1_Score(fitted, as.factor(test.data$outcome), "RFE Regression Test", 0.5)
EvaluateModel(fitted,  as.factor(test.data$outcome), "RFE Regression Test", 0.5)
test.data <-test.dataOrig.random
fitted <- predict(model.rf,test.data)
Evaluate_F1_Score(fitted, as.factor(test.data$outcome), "RFE Regression Test", 0.5)
EvaluateModel(fitted,  as.factor(test.data$outcome), "RFE Regression Test", 0.5)
str(test.data)
summary(fitted)
table(fitted)
pred <- factor(ifelse(fitted >= 0.5, 1, 0))
pred
table(pred)
scoreDS <- read.csv("scoringData.csv",header=TRUE)
str(scoreDS)
summary(scoreDS)
scoreDS[is.na(scoreDS)] <- 1
scoreDSNoNA <- scoreDS
str(scoreDSNoNA)
columns <- c( 'fqhc','tier','pcp_lookback','family_assignment','kid',
'is_ped','same_gender','same_language','same_address','distance','visit_count')
scoreDSNoIndex <- scoreDSNoNA %>% select(columns)
str(scoreDSNoIndex)
scoreDS <- read.csv("scoringData.csv",header=TRUE)
str(scoreDS)
scoreDS <- scoreDS %>% select(-c("claims_daysaway"))
str(scoreDS)
str(test.data)
columns <- c( 'fqhc','tier','pcp_lookback','family_assignment','kid',
'is_ped','same_gender','same_language','same_address','distance','visit_count')
scoreDSNoIndex <- scoreDSNoNA %>% select(columns)
str(scoreDSNoIndex)
scores <-  predict(model.rf,scoreDSNoIndex)
pred <- factor(ifelse(fitted >= 0.5, 1, 0))
pred
table(pred)
pred <- factor(ifelse(scores >= 0.5, 1, 0))
pred
table(pred)
str(scoreDS)
finalDS <- scoreDSNoNA[,testindex]
finalDS <- scoreDSNoNA[,$testindex]
finalDS <- scoreDSNoNA[,1]
finalDS
View(scoreDS)
finalDS$pred <- pred
finalDS
pred
class(pred)
finalDS
finalDS <- scoreDSNoNA[,1]
finalDS
class[finalDS]
finalDS <- as.data.frame(pred)
str(finalDS)
pred <- ifelse(scores >= 0.5, 1, 0)
pred <- ifelse(scores >= 0.5, 1, 0)
pred
finalDS <- rbind(scoreDSNoNA[,1],as.data.frame(pred))
str(finalDS)
finalDS <- scoreDSNoNA[,1]
finalDS$pred <- as.data.frame(pred)
finalDS <- scoreDSNoNA[,1]
predDS <- as.data.frame(pred)
View(predDS)
finalDS <- as.data.frame(scoreDSNoNA[,1])
predDS <- as.data.frame(pred)
View(finalDS)
submitDS <- rbind(finalDS,predDS))
submitDS <- rbind(finalDS,predDS)
submitDS <- data.frame(scoreDSNoNA[,1], pred)
str(submitDS)
table(pred)
write.csv(submitDS,file="Submission_withR.csv",row.names=FALSE, na="")
